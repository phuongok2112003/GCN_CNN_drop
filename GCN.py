import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from imblearn.over_sampling import SMOTE
import numpy as np
from sklearn.model_selection import train_test_split
import random

# === 1. Äáº·t seed Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n ===
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# === 2. Äá»‹nh nghÄ©a mÃ´ hÃ¬nh GCN cho cáº£ node vÃ  edge ===
class GraphModel(nn.Module):
    def __init__(self, node_input_dim, node_hidden_dim, node_output_dim, 
                 edge_input_dim, edge_hidden_dim, edge_output_dim):
        super(GraphModel, self).__init__()
        
        # GCN cho node
        self.node_gcn1 = GCNConv(node_input_dim, node_hidden_dim)
        self.node_gcn2 = GCNConv(node_hidden_dim, node_output_dim)
        
        # GCN cho edge
        self.edge_gcn1 = GCNConv(edge_input_dim, edge_hidden_dim)
        self.edge_gcn2 = GCNConv(edge_hidden_dim, edge_output_dim)

        # Fully Connected Ä‘á»ƒ káº¿t há»£p node & edge
        self.fc = nn.Linear(node_output_dim + edge_output_dim, node_output_dim)

    def forward(self, data):
        # ğŸ”¹ TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« node báº±ng GCN
        node_features = F.sigmoid(self.node_gcn1(data.x, data.edge_index))
        node_features = self.node_gcn2(node_features, data.edge_index)

        # ğŸ”¹ TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« edge báº±ng GCN
        if data.edge_attr is not None:
            edge_features = F.sigmoid(self.edge_gcn1(data.edge_attr, data.edge_index))
            edge_features = self.edge_gcn2(edge_features, data.edge_index)
        else:
            edge_features = torch.zeros((data.edge_index.shape[1], 16), device=data.x.device)

        # ğŸ”¹ Pooling Ä‘á»ƒ gom Ä‘áº·c trÆ°ng cá»§a node vÃ  edge thÃ nh vector Ä‘áº¡i diá»‡n cho Ä‘á»“ thá»‹
        node_representation = global_mean_pool(node_features, data.batch)
        edge_representation = global_mean_pool(edge_features, data.batch[data.edge_index[0]])

        # ğŸ”¹ Káº¿t há»£p Ä‘áº·c trÆ°ng cá»§a node & edge
        graph_representation = torch.cat([node_representation, edge_representation], dim=-1)
        graph_representation = self.fc(graph_representation)

        return graph_representation

# === 3. Load dá»¯ liá»‡u Ä‘á»“ thá»‹ ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
graphs = torch.load('graphs.pt',weights_only=False)

print("Sá»‘ lÆ°á»£ng Ä‘á»“ thá»‹:", len(graphs))

# CÃ¢n báº±ng dá»¯ liá»‡u
graphs_0 = [g for g in graphs if g.y.item() == 0]
graphs_1 = [g for g in graphs if g.y.item() == 1]
min_size = min(len(graphs_0), len(graphs_1))
print(f"CÃ¢n báº±ng táº­p dá»¯ liá»‡u vá» {min_size} máº«u má»—i lá»›p...")

balanced_graphs = graphs_0[:min_size] + graphs_1[:min_size]
remaining_graphs = graphs_0[min_size:] + graphs_1[min_size:]
random.shuffle(balanced_graphs)
random.shuffle(remaining_graphs)

# === 4. Chia Train/Test ===
print("Chia táº­p train (80%) vÃ  test (20%)...")
train_graphs, test_graphs = train_test_split(
    balanced_graphs, test_size=0.2, random_state=42, stratify=[g.y.item() for g in balanced_graphs]
)

# ThÃªm dá»¯ liá»‡u dÆ° vÃ o táº­p train vÃ  test
train_graphs += remaining_graphs[:18000]
test_graphs += remaining_graphs[18000:]

# === 5. TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« mÃ´ hÃ¬nh ===
model = GraphModel(node_input_dim=50, node_hidden_dim=64, node_output_dim=32,
                   edge_input_dim=50, edge_hidden_dim=32, edge_output_dim=16).to(device)
model.eval()

def extract_features(graphs):
    loader = DataLoader(graphs, batch_size=1, shuffle=False)
    features, labels = [], []
    print("Sá»‘ lÆ°á»£ng máº«u trong loader:", len(loader))
    
    with torch.no_grad():
        for data in loader:
            data = data.to(device)
            graph_features = model(data)
            features.append(graph_features.cpu().numpy())
            labels.append(data.y.cpu().numpy())

    X = np.vstack(features)
    y = np.hstack(labels)
    return X, y

# TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng cho táº­p train vÃ  test
X_train, y_train = extract_features(train_graphs)
X_test, y_test = extract_features(test_graphs)
print(X_train.shape)
# === 6. CÃ¢n báº±ng táº­p train báº±ng SMOTE ===
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print("TrÆ°á»›c SMOTE:", np.bincount(y_train))
print("Sau SMOTE:", np.bincount(y_train_resampled))

# === 7. Chuyá»ƒn láº¡i thÃ nh dá»¯ liá»‡u Ä‘á»“ thá»‹ PyG ===
def convert_to_graphs(X, y):
    new_graphs = []
    for i in range(len(X)):
        graph_data = Data(
            x=torch.tensor(X[i], dtype=torch.float).unsqueeze(0),
            y=torch.tensor([y[i]], dtype=torch.long)
        )
        new_graphs.append(graph_data)
    return new_graphs

train_graphs_resampled = convert_to_graphs(X_train_resampled, y_train_resampled)
test_graphs_final = convert_to_graphs(X_test, y_test)

# === 8. LÆ°u dá»¯ liá»‡u sau khi xá»­ lÃ½ ===
torch.save(train_graphs_resampled, 'train_graphs.pt')
torch.save(test_graphs_final, 'test_graphs.pt')
print("ÄÃ£ lÆ°u train_graphs.pt vÃ  test_graphs.pt.")
